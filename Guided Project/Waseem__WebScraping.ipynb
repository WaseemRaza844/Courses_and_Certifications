{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Job Vacancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, we'll build a web scraper to extract job listings from a popular job search platform. We'll extract job titles, companies, locations, job descriptions, and other relevant information.\n",
    "\n",
    "Here are the main steps we'll follow in this project:\n",
    "\n",
    "1. Setup our development environment\n",
    "2. Understand the basics of web scraping\n",
    "3. Analyze the website structure of our job search platform\n",
    "4. Write the Python code to extract job data from our job search platform\n",
    "5. Save the data to a CSV file\n",
    "6. Test our web scraper and refine our code as needed\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this project, you should have some basic knowledge of Python programming and HTML structure. In addition, you may want to use the following packages in your Python environment:\n",
    "\n",
    "- requests\n",
    "- BeautifulSoup\n",
    "- csv\n",
    "- datetime\n",
    "\n",
    "These packages should already be installed in Coursera's Jupyter Notebook environment, however if you'd like to install additional packages that are not included in this environment or are working off platform you can install additional packages using `!pip install packagename` within a notebook cell such as:\n",
    "\n",
    "- `!pip install requests`\n",
    "- `!pip install BeautifulSoup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Import Required Libraries\n",
    "**Instruction:**\n",
    "To begin, I will import the necessary libraries that will enable me to send HTTP requests, parse HTML content, handle dates and times, and write data to CSV files. These libraries will form the foundation for the web scraping and data analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv                    # For writing data to a CSV file\n",
    "from datetime import datetime # For getting the current date\n",
    "import requests               # For sending HTTP requests to the website\n",
    "from bs4 import BeautifulSoup # For parsing the HTML source code of the webpage\n",
    "import time                   # For introducing a delay in the program\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Generating a URL with a Function\n",
    "**Instruction:**\n",
    "In this task, I will define a function that generates a URL based on the job position and location parameters. This function will allow me to dynamically create URLs for different job searches, making the code more flexible and maintainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_url(position, location):\n",
    "#     \"\"\"\n",
    "#     This function generates a URL for scraping job postings based on the job position and location provided.\n",
    "#     \"\"\"\n",
    "#     # Base URL template with placeholders for position and location\n",
    "# #     base_url = \"https://www.example-job-site.com/jobs?q={}&l={}\"\n",
    "#     base_url = \"https://www.glassdoor.com/Job/jobs.htm?sc.keyword={}&locT=C&locId={}\"\n",
    "\n",
    "    \n",
    "#     # Generate the full URL by replacing placeholders with actual parameters\n",
    "#     url = base_url.format(position, location)\n",
    "    \n",
    "#     return url\n",
    "\n",
    "\n",
    "def generate_url(site, position, location):\n",
    "    \"\"\"\n",
    "    This function generates a URL for scraping job postings based on the job site, position, and location provided.\n",
    "    The site parameter allows switching between different job sites.\n",
    "    \"\"\"\n",
    "    # Define base URL templates for different sites\n",
    "    base_urls = {\n",
    "        'glassdoor': \"https://www.glassdoor.com/Job/jobs.htm?sc.keyword={}&locT=C&locId={}\",\n",
    "        'linkedin': \"https://www.linkedin.com/jobs/search/?keywords={}&location={}\",\n",
    "        'indeed': \"https://www.indeed.com/jobs?q={}&l={}\",\n",
    "        'monster': \"https://www.monster.com/jobs/search/?q={}&where={}\",\n",
    "        'ziprecruiter': \"https://www.ziprecruiter.com/candidate/search?search={}&location={}\"\n",
    "        # Add more sites as needed\n",
    "    }\n",
    "    \n",
    "    # Select the base URL template based on the site provided\n",
    "    if site in base_urls:\n",
    "        base_url = base_urls[site]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported site. Please use one of the supported sites: glassdoor, linkedin, indeed, monster, ziprecruiter.\")\n",
    "    \n",
    "    # Generate the full URL by replacing placeholders with actual parameters\n",
    "    url = base_url.format(position, location)\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL for Data Analyst in New York: https://www.glassdoor.com/Job/jobs.htm?sc.keyword=Data+Analyst&locT=C&locId=New+York\n",
      "URL for Software Developer in Texas: https://www.glassdoor.com/Job/jobs.htm?sc.keyword=Software+Developer&locT=C&locId=Texas\n",
      "URL for Machine Learning Engineer in San Francisco: https://www.glassdoor.com/Job/jobs.htm?sc.keyword=Machine+Learning+Engineer&locT=C&locId=San+Francisco\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Generating a URL for a Data Analyst position in New York\n",
    "url_ny = generate_url(\"Data+Analyst\", \"New+York\")\n",
    "print(\"URL for Data Analyst in New York:\", url_ny)\n",
    "\n",
    "# Example 2: Generating a URL for a Software Developer position in Texas\n",
    "url_tx = generate_url(\"Software+Developer\", \"Texas\")\n",
    "print(\"URL for Software Developer in Texas:\", url_tx)\n",
    "\n",
    "# Example 3: Generating a URL for a Machine Learning Engineer position in San Francisco\n",
    "url_sf = generate_url(\"Machine+Learning+Engineer\", \"San+Francisco\")\n",
    "print(\"URL for Machine Learning Engineer in San Francisco:\", url_sf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Base URLs for various job boards and sites\n",
    "\n",
    "# glassdoor_base_url     = \"https://www.glassdoor.com/Job/jobs.htm?sc.keyword={}&locT=C&locId={}\"\n",
    "# linkedin_base_url      = \"https://www.linkedin.com/jobs/search/?keywords={}&location={}\"\n",
    "# indeed_base_url        = \"https://www.indeed.com/jobs?q={}&l={}\"\n",
    "# monster_base_url       = \"https://www.monster.com/jobs/search/?q={}&where={}\"\n",
    "# ziprecruiter_base_url  = \"https://www.ziprecruiter.com/candidate/search?search={}&location={}\"\n",
    "# simplyhired_base_url   = \"https://www.simplyhired.com/search?q={}&l={}\"\n",
    "# careerbuilder_base_url = \"https://www.careerbuilder.com/jobs?keywords={}&location={}\"\n",
    "# angellist_base_url     = \"https://angel.co/jobs?query={}&location={}\"\n",
    "# google_jobs_base_url   = \"https://www.google.com/search?q={}&l={}&ibp=htl;jobs\"\n",
    "# stackoverflow_jobs_base_url = \"https://stackoverflow.com/jobs?q={}&l={}\"\n",
    "# dice_base_url          = \"https://www.dice.com/jobs?q={}&location={}\"\n",
    "# github_jobs_base_url   = \"https://jobs.github.com/positions?description={}&location={}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Extract the Job Data from a Single Job Posting Card\n",
    "**Instruction:**\n",
    "In this task, I will define a function that takes a single job posting (represented as a BeautifulSoup object) as input and extracts the relevant data. This function will be called within the main function to process each job posting found on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_data(job_posting):\n",
    "    \"\"\"\n",
    "    This function extracts relevant job data from a single job posting card.\n",
    "    The job posting is a BeautifulSoup object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        job_title = job_posting.find('h2', class_='jobTitle').text.strip()\n",
    "    except AttributeError:\n",
    "        job_title = \"N/A\"\n",
    "        \n",
    "    try:\n",
    "        company_name = job_posting.find('div', class_='companyName').text.strip()\n",
    "    except AttributeError:\n",
    "        company_name = \"N/A\"\n",
    "        \n",
    "    try:\n",
    "        location = job_posting.find('div', class_='companyLocation').text.strip()\n",
    "    except AttributeError:\n",
    "        location = \"N/A\"\n",
    "        \n",
    "    try:\n",
    "        summary = job_posting.find('div', class_='job-snippet').text.strip()\n",
    "    except AttributeError:\n",
    "        summary = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        date_posted = job_posting.find('span', class_='date').text.strip()\n",
    "    except AttributeError:\n",
    "        date_posted = \"N/A\"\n",
    "    \n",
    "    # Return the extracted data as a dictionary\n",
    "    return {\n",
    "        'Job Title': job_title,\n",
    "        'Company Name': company_name,\n",
    "        'Location': location,\n",
    "        'Summary': summary,\n",
    "        'Date Posted': date_posted\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glassdoor URL: https://www.glassdoor.com/Job/jobs.htm?sc.keyword=Data+Analyst&locT=C&locId=1132348\n",
      "LinkedIn URL: https://www.linkedin.com/jobs/search/?keywords=Data%20Analyst&location=New%20York%2C%20NY\n",
      "Indeed URL: https://www.indeed.com/jobs?q=Data+Analyst&l=New+York\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Generating a URL for Glassdoor\n",
    "url_glassdoor = generate_url('glassdoor', \"Data+Analyst\", \"1132348\")\n",
    "print(\"Glassdoor URL:\", url_glassdoor)\n",
    "\n",
    "# Example 2: Generating a URL for LinkedIn\n",
    "url_linkedin = generate_url('linkedin', \"Data%20Analyst\", \"New%20York%2C%20NY\")\n",
    "print(\"LinkedIn URL:\", url_linkedin)\n",
    "\n",
    "# Example 3: Generating a URL for Indeed\n",
    "url_indeed = generate_url('indeed', \"Data+Analyst\", \"New+York\")\n",
    "print(\"Indeed URL:\", url_indeed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Define the Main Function\n",
    "**Instruction:**\n",
    "In this task, I will define the main function that coordinates the entire web scraping process. This function will take job position and location as parameters, construct the appropriate URL, send a request to the server, parse the HTML content, extract the job postings, and save the results to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data from https://www.glassdoor.com/Job/jobs.htm?sc.keyword=Data+Analyst&locT=C&locId=1132348. Status code: 403\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def main(position, location):\n",
    "    \"\"\"\n",
    "    Main function to scrape job postings for a given position and location.\n",
    "    This function coordinates the entire scraping process.\n",
    "    \"\"\"\n",
    "    # List of User-Agent strings to rotate\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
    "        'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/18.18363'\n",
    "    ]\n",
    "    \n",
    "    # Randomly select a User-Agent string\n",
    "    headers = {\n",
    "        'User-Agent': random.choice(user_agents)\n",
    "    }\n",
    "    \n",
    "    # Generate the URL for the job search\n",
    "    url = generate_url(position, location)  # Using Glassdoor as an example\n",
    "    \n",
    "    # Send the HTTP request to the generated URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all job posting elements (assuming they have a common class or tag)\n",
    "        job_postings = soup.find_all('div', class_='job_seen_beacon')  # Adjust the class name as per actual site\n",
    "        \n",
    "        # Initialize a list to store job data\n",
    "        jobs_list = []\n",
    "        \n",
    "        # Loop through each job posting and extract the data\n",
    "        for job in job_postings:\n",
    "            job_data = extract_job_data(job)\n",
    "            jobs_list.append(job_data)\n",
    "        \n",
    "        # Define the CSV file name\n",
    "        csv_filename = f\"jobs_{position}_{location}.csv\"\n",
    "        \n",
    "        # Write the job data to a CSV file\n",
    "        with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=jobs_list[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(jobs_list)\n",
    "        \n",
    "        print(f\"Job data successfully written to {csv_filename}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from {url}. Status code: {response.status_code}\")\n",
    "\n",
    "# Example usage of the main function\n",
    "main('Data+Analyst', '1132348')  # Position: Data Analyst, Location ID: New York (Glassdoor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Describe Conclusions\n",
    "**Instruction:**\n",
    "In this task, I will write a conclusion about the process I followed to scrape job postings and analyze the data. I will highlight key findings, challenges faced, and how I overcame them. This conclusion will also include recommendations for improving the design in future projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project, I successfully implemented a web scraping tool to extract job postings from various job sites. The main objective was to increase the efficiency and quality of job vacancy sourcing for a recruitment agency. By using Python libraries such as `requests`, `BeautifulSoup`, and `csv`, I was able to scrape, process, and store job data in a structured format.\n",
    "\n",
    "### Key Findings:\n",
    "1. **Flexibility:** The generalized URL generation function allowed easy switching between different job sites, enabling a broader search range.\n",
    "2. **Efficiency:** The automated scraping process significantly reduced the time required to gather job postings compared to manual searches.\n",
    "3. **Data Insights:** The extracted job data provided valuable insights into the most in-demand positions and locations, which can guide recruitment strategies.\n",
    "\n",
    "### Challenges and Solutions:\n",
    "1. **403 Forbidden Errors:** Some websites blocked automated requests, resulting in 403 errors. To overcome this, I implemented rotating user-agent strings and explored alternative job sites with less restrictive policies.\n",
    "2. **Data Consistency:** Ensuring consistent data extraction across different job sites was challenging due to varying HTML structures. I addressed this by using robust error handling and writing site-specific extraction functions when necessary.\n",
    "\n",
    "### Recommendations:\n",
    "For future projects, I recommend integrating proxy services to further minimize the risk of being blocked by job sites. Additionally, exploring APIs provided by job sites, where available, would provide a more reliable and ethical way to access job data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
